---
title: "Predicting Genetic Disorders"
author:
  Emma Oo^[University of San Diego, eoo@sandiego.edu], Sindhu Bhattarai^[University of San Diego, sbhattarai@sandiego.edu], Dave Friesen^[University of San Diego, dfriesen@sandiego.edu]
date: "06/27/2022"
geometry: "left=2cm,right=2cm,top=2cm,bottom=2cm"
output:
  html_document:
    css: "style.css"
  pdf_document: default
---

<style>
.main-container {
  max-width: 1024px;
}
</style>


```{r setup, echo = FALSE, message = FALSE}
# Load R libraries
library(caret)
library(cowplot)
library(DescTools)
library(dplyr)
library(e1071)
library(ggplot2)
library(MASS)
library(patchwork)
library(pROC)
library(randomForest)

# Expand output width and minimize exp notation
options(width = 150)
options(scipen = 100)
options(digits = 1)

# Set style defaults
knitr::opts_chunk$set(class.source = "source")
knitr::opts_chunk$set(class.output = "output")
knitr::opts_chunk$set(fig.width = 8.5, fig.height = (8.5 * .7), fig.align = "center")
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(comment = NA)

# Set conditional model eval so can run < full set at a time for verification
lda_eval = TRUE
lr_eval = TRUE
nsc_eval = TRUE
rf_eval = TRUE
cart_eval = TRUE
bt_eval = TRUE
knn_eval = TRUE
```


### Data Load and Validation

```{r data_load_validation}
# Load dataset(s)
gd_df <- read.csv("../data/train_genetic_disorders.csv", header = TRUE)

# Data validation and understanding, including structure, content, and statistical characteristics covered below
```


###### Data Structure Review

```{r data_structure}
# Summarize base dataset and [optionally] sample rows
str(gd_df)
#head(gd_df, 3)
```


###### Preliminary Feature Reduction (clearly n/a to Objective and Hypothesis)

```{r prelim_feature_reduction}
# Define n/a columns and subset dataframe; Note retaining "some" informational variables like "Institute.Name" for
#   possible descriptive analytic purposes
drop_cols <- c("Patient.Id",
               "Patient.First.Name",
               "Family.Name",
               "Father.s.name",
               "Institute.Name",
               "Location.of.Institute",
               "Status",
               "Test.1",
               "Test.2",
               "Test.3",
               "Test.4",
               "Test.5",
               "Parental.consent",
               "Birth.asphyxia",
               "Place.of.birth")
gd_df <- gd_df[ , !(names(gd_df) %in% drop_cols)]
```


###### Class Target and Label Review

```{r class_target_label_review}
# Check for missing labels; set aside where missing
missing_target <- which(is.na(gd_df$Disorder.Subclass) | (gd_df$Disorder.Subclass == ""))
cat("Rows pre-subset for missing labels: ", format(nrow(gd_df), format = "d", big.mark = ","), sep = "")
gd_hold_df <- gd_df[missing_target, ]
gd_df <- gd_df[-missing_target, ]
cat("Deleted rows with missing labels: ", format(nrow(gd_hold_df), format = "d", big.mark = ","), sep = "")
cat("Remaining rows (labeled): ", format(nrow(gd_df), format = "d", big.mark = ","), sep = "")

# Show frequency distribution for [prospective] target class(es)
show_frequency <- function(desc, c) {
  t <- as.data.frame(prop.table(table(c)))
  colnames(t) <- c("Class", "Frequency")
  cat(desc, "\n"); print(t[order(-t$Freq, t$Class), 1:2], row.names = FALSE)
}
show_frequency("Pre-Split Frequency Distribution", gd_df$Disorder.Subclass)

# Move the target class to "top" of dataframe so column removals don't impact
gd_df <- gd_df[ , c(ncol(gd_df), 1:(ncol(gd_df) - 1))]
target_col = 1

# Clean (prelim) target class values
gd_df$Disorder.Subclass <- gsub("'", "", gd_df$Disorder.Subclass, fixed = TRUE)
gd_df$Disorder.Subclass <- gsub(" ", ".", gd_df$Disorder.Subclass, fixed = TRUE)
gd_df$Disorder.Subclass <- gsub("-", ".", gd_df$Disorder.Subclass, fixed = TRUE)
```


### Data Partitioning

```{r data_partitioning}
# Split data 80/20 train/test, using caret's inherent stratified split to compensate for class imbalance
set.seed(1)
train_index <- createDataPartition(gd_df$Disorder.Subclass, times = 1, p = 0.80, list = FALSE)
train_df <- gd_df[train_index, ]
test_df <- gd_df[-train_index, ]
show_frequency("Post-Split Frequency Distribution (Train)", train_df$Disorder.Subclass)
```


### Data Cleaning (and reduction)


###### Data (Sample) Characteristic Review for Pre-Processing
(Suppressing custom code for simplicity)

```{r data_univariate, echo = FALSE}
# Note this function is generic and doesn't look for more intelligent "blank" values like "no record",
#   "not available", etc.
is_blank <- function(x) {
  classof_x <- class(x)
  result <-
    !is.na(x) &
    (((classof_x == "character") & (x == "")) |
     ((classof_x %in% c("integer", "numeric")) & (x == 0)))
  return(result)
}

# Function to format percentages (only when value exists)
format_percent <- function(x) {
  result <- formatC(x * 100, digits = 0, width = 5, format = "d", zero.print = FALSE)
  if (x != 0) result <- paste(result, "%", sep = "")
  return(result)  
}

# Function to not output NaNs from third-party functions in lapply() below
nan_replace_0 <- function(x) {
  if (is.nan(x)) result <- 0 else result = x
  return(result)
}

# Function to Generate a summary of base dataset
univariate <- function(df) {
  rowcount <- nrow(df)
  ua <- do.call(rbind, lapply(df, function(x) c(
    colnames(x),
    class(x),
    format_percent(sum(is.na(x)) / rowcount),
    format_percent(sum(is_blank(x)) / rowcount),
    formatC(length(unique(na.omit(x))),
            digits = 0, width = 7, format = "d", big.mark = ",", zero.print = FALSE),
    formatC(ifelse(is.numeric(x), min(na.omit(x)), 0),
            digits = ifelse(is.double(x), 3, 0), width = 7, format = "f", big.mark = ",", zero.print = FALSE),
    formatC(ifelse(is.numeric(x), max(na.omit(x)), 0),
            digits = ifelse(is.double(x), 3, 0), width = 7, format = "f", big.mark = ",", zero.print = FALSE),
    formatC(ifelse(is.double(x), mean(na.omit(x)), 0),
            digits = 3, width = 7, format = "f", big.mark = ",", zero.print = FALSE),
    formatC(ifelse(is.numeric(x), median(na.omit(x)), 0),
            digits = ifelse(is.double(x), 3, 0), width = 7, format = "f", big.mark = ",", zero.print = FALSE),
    format(ifelse(is.numeric(x),
           ifelse(na.omit(x) < (quantile(na.omit(x), 0.25) - (1.5 * IQR(na.omit(x)))), "Yes", "No"), ""),
           justify = "centre", width = 8, format = "s"),
    format(ifelse(is.numeric(x),
           ifelse(na.omit(x) > (quantile(na.omit(x), 0.75) - (1.5 * IQR(na.omit(x)))), "Yes", "No"), ""),
           justify = "centre", width = 8, format = "s"),
    formatC(ifelse(is.numeric(x), nan_replace_0(skewness(na.omit(x))), 0),
            digits = 3, width = 8, format = "f", zero.print = FALSE),
    formatC(ifelse(is.numeric(x), nan_replace_0(kurtosis(na.omit(x))), 0),
            digits = 3, width = 8, format = "f", zero.print = FALSE))))
  colnames(ua) <- c(
    "Type",
    format("NA", justify = "right", width = 6),
    format("BlankZ", justify = "right", width = 6),
    format("Unique", justify = "right", width = 7),
    format("Min", justify = "right", width = 7),
    format("Max", justify = "right", width = 7),
    format("Mean", justify = "right", width = 7),
    format("Median", justify = "right", width = 7),
    format("Outlier<", justify = "centre", width = 8),
    format(">Outlier", justify = "centre", width = 8),
    format("Kurtosis", justify = "right", width = 8),
    format("Skewness", justify = "right", width = 8))
  row.names(ua) <- lapply(row.names(ua),
                          function(x) if (nchar(x) > 20) return(paste(substr(x, 1, 17), "...", sep = ""))
                          else return(x))
  { cat(
    "Summary Univariate Analysis (",
    formatC(rowcount, big.mark = ","), " observations)\n",
    sep = "")
    print(noquote(ua))
  }
}
```


```{r data_characteristics}
# Generate a summary (cursory) view of base dataset for initial understanding and pre-processing direction
univariate(train_df)
```


###### Missing Values

```{r missing_values}
# Impute basic integer values with medians
medianf <- function(x) {
 result <- median(x, na.rm = TRUE)
 if (is.integer(x))
   result <- as.integer(result)
 return(result)
}
median_cols = c("Patient.Age", "Mother.s.age", "Father.s.age", "No..of.previous.abortion")
for (n in median_cols) {
  train_df[n][is.na(train_df[n])] <- apply(train_df[n], 2, medianf)
  test_df[n][is.na(test_df[n])] <- apply(test_df[n], 2, medianf)
}

# Impute categorical blanks with common "notprovided"; note we could also impute these with categorical mode,
#   or most frequent categorical value of each column using the cmode() function below
cols_tofill <- c("Inherited.from.father",
                 "Maternal.gene",
                 "Respiratory.Rate..breaths.min.",
                 "Heart.Rate..rates.min",
                 "Follow.up",
                 "Gender",
                 "Autopsy.shows.birth.defect..if.applicable.",
                 "Folic.acid.details..peri.conceptional.",
                 "H.O.serious.maternal.illness",
                 "H.O.radiation.exposure..x.ray.",
                 "H.O.substance.abuse",
                 "Assisted.conception.IVF.ART",
                 "History.of.anomalies.in.previous.pregnancies",
                 "Birth.defects",
                 "Blood.test.result")
train_df[cols_tofill][train_df[cols_tofill] == ""] <- "notprovided"
test_df[cols_tofill][test_df[cols_tofill] == ""] <- "notprovided"

cmode <- function(x) {
  uniqx <- unique(na.omit(x))
  uniqx[which.max(tabulate(match(x, uniqx)))]
}

# Impute what appear to be masked "flag" columns iwth placeholder -1 values. . .
flag_cols <- c("Symptom.1", "Symptom.2", "Symptom.3", "Symptom.4", "Symptom.5")
train_df[flag_cols][is.na(train_df[flag_cols])] <- as.integer(-1)
test_df[flag_cols][is.na(test_df[flag_cols])] <- as.integer(-1)

# Impute mean for one numeric column
train_df$White.Blood.cell.count..thousand.per.microliter.[is.na(train_df$White.Blood.cell.count..thousand.per.microliter.)] <-
  mean(train_df$White.Blood.cell.count..thousand.per.microliter., na.rm = TRUE)
test_df$White.Blood.cell.count..thousand.per.microliter.[is.na(test_df$White.Blood.cell.count..thousand.per.microliter.)] <-
  mean(test_df$White.Blood.cell.count..thousand.per.microliter., na.rm = TRUE)

# Note not using knnImpute for the limited number of numerical [prospective] features given that it
#   centers/scales, which is illogical for the values in this dataset
#pp <- preProcess(train_df[ , -target_col, drop = FALSE], method = "knnImpute", k = 10)
#train_df[ , -target_col] <- predict(pp, train_df[ , -target_col, drop = FALSE])
#test_df[ , -target_col] <- predict(pp, test_df[ , -target_col, drop = FALSE])

# Last on the list: Genetic.Disorder - we're not classifying to this but it is relevant/informational as a
#   superclass to the target Disorder.Subclass and shuold ultimately be imputed using similar Disorder.Subclass
#   observations which do have valid Genetic.Disorder values
```


###### Feature Updates (including variable types/formats, names)

```{r feature_updates}
# Re-type variables
factor_cols <- c("Disorder.Subclass",
                 "Genes.in.mother.s.side",
                 "Inherited.from.father",
                 "Maternal.gene",
                 "Paternal.gene",
                 "Respiratory.Rate..breaths.min.",
                 "Heart.Rate..rates.min",
                 "Follow.up",
                 "Gender",
                 "Autopsy.shows.birth.defect..if.applicable.",
                 "Folic.acid.details..peri.conceptional.",
                 "H.O.serious.maternal.illness",
                 "H.O.radiation.exposure..x.ray.",
                 "H.O.substance.abuse",
                 "Assisted.conception.IVF.ART",
                 "History.of.anomalies.in.previous.pregnancies",
                 "Birth.defects",
                 "Blood.test.result",
                 "Genetic.Disorder")
train_df[factor_cols] <- lapply(train_df[factor_cols], factor)
test_df[factor_cols] <- lapply(test_df[factor_cols], factor)
# Note dummy variables may be introduced below (model-dependent)

# Simplify variable naming
rename_cols <- c("Disorder_Subclass",
                 "Patient_Age",
                 "Genes_mothers_side",
                 "Genes_fathers_side",
                 "Maternal_gene",
                 "Paternal_gene",
                 "Blood_cell_count",
                 "Mothers_age",
                 "Fathers_age",
                 "Respiratory_Rate",
                 "Heart_Rate",
                 "Follow_up",
                 "Gender",
                 "Autopsy_birth_defect",
                 "Folic_acid_conceptional",
                 "HO_maternal_illness",
                 "HO_radiation_exposure",
                 "HO_substance_abuse",
                 "Assisted_conception",
                 "Previous_pregnancies_issues",
                 "Previous_abortions",
                 "Birth_defects",
                 "White_Blood_cell_count",
                 "Blood_test_result",
                 "Symptom_1",
                 "Symptom_2",
                 "Symptom_3",
                 "Symptom_4",
                 "Symptom_5",
                 "Genetic_Disorder")
colnames(train_df) <- rename_cols
colnames(test_df) <- rename_cols
```


###### Zero/Near-Zero Variances

```{r near_zero_z_variances}
# n/a for this dataset
```


###### Duplicate Values

```{r duplicate_values}
# n/a for this dataset
```


###### "Noisy" Data

```{r noisy_data}
# n/a for this dataset
```


### Data Transformation


###### Centering/Scaling (standardizing/normalizing)

```{r centering_scaling}
# n/a for this dataset
```


###### Statistical Characteristics (including distribution, skewness, outliers)

```{r statistical_characteristics}
# Generate updated summary of base dataset which includes these characteristics
univariate(train_df)
#summary(train_df)
```

```{r statistical_distributions}
# Generate histograms across predictors and target
pred_for_hist <- train_df[ , 2:29]
pred_for_hist <- pred_for_hist %>% mutate_if(is.character, as.numeric)
pred_for_hist <- pred_for_hist %>% mutate_if(is.factor, as.numeric)
par(mfrow = c(3, 3))
for (i in 1:ncol(pred_for_hist))
  hist(pred_for_hist[ , i], xlab = names(pred_for_hist[i]), main = paste(names(pred_for_hist[i]), "Histogram"), col = "blue")  
```


```{r statistical_outlers}
# Generate boxplot(s)
par(mar = c(10, 2, 1, 1))
boxplot(train_df, las = 2, col = c("turquoise", "skyblue"), main = "Distribution and Outlier Review", ylab = "Frequency")
```


###### Other Feature Engineering (transformation, aggregation, enrichment)

```{r other_feature_engineering}
# n/a for this dataset
```


### Multivariate Analysis (and reduction)


###### Collinearity and Dependencies

```{r collinearity_and_dependencies}
# Calculate Cramer's V "measure of association" between nominal factor variables (uses Chi-square statistic)
cscorr <- PairApply(train_df[ , sapply(train_df, is.factor)], CramerV, symmetric = TRUE)

# Shorten variable names for ease of reviewing output matrix
rn <- rownames(cscorr)
for (n in 1:length(rownames(cscorr))) {
  rn[n] <- paste(rownames(cscorr)[n], " (", AscToChar(64 + n), ")", sep = "")
  rownames(cscorr)[n] <- paste(AscToChar(64 + n))
}
for (n in 1:length(colnames(cscorr)))
  colnames(cscorr)[n] <- paste(AscToChar(64 + n))

# Show master list of variable names along with output ("correlation") matrix
cat(rn, sep = "\n")
cscorr
```


```{r target_predictor_correlation}
# Per hypothesis, relate (visualize) target with maternal and paternal genes to understand more direct relationship
p1 <- ggplot(train_df, aes(x = train_df$Patient_Age, fill = train_df$Disorder_Subclass)) + geom_bar() + theme_classic() + scale_fill_hue(c = 60, l = 80)
p2 <- ggplot(train_df, aes(x = train_df$Maternal_gene, fill = train_df$Disorder_Subclass)) + geom_bar() + theme_classic() + scale_fill_hue(c = 60, l = 80)
p3 <- ggplot(train_df, aes(x = train_df$Paternal_gene, fill = train_df$Disorder_Subclass)) + geom_bar() + theme_classic() + scale_fill_hue(c = 60, l = 80)
p1 + p2 / p3
```


###### Predictor Transformations (e.g., PCA)

```{r predictor_transformations}
```


### Modeling


```{r model_preparation}
# Convert factors to dummies (retaining non-factors and also keeping the target as a factor)
dummies <- dummyVars(Disorder_Subclass ~. , data = train_df[ , sapply(train_df, is.factor)])
train_df <- cbind(Disorder_Subclass = train_df$Disorder_Subclass, train_df[ , !sapply(train_df, is.factor)], data.frame(predict(dummies, newdata = train_df)))
dummies <- dummyVars(Disorder_Subclass ~. , data = test_df[ , sapply(test_df, is.factor)])
test_df <- cbind(Disorder_Subclass = test_df$Disorder_Subclass, test_df[ , !sapply(test_df, is.factor)], data.frame(predict(dummies, newdata = test_df)))

# Create common control for models
set.seed(1)
fit_control <- trainControl(method = "cv",
                            savePredictions = "all",
                            classProbs = TRUE,
                            summaryFunction = multiClassSummary)
```


###### Linear Discriminate Analysis Model

```{r lda_model, eval = lda_eval}
# Train LDA model
set.seed(476)
lda_fit <- train(x = train_df[ , -target_col, drop = FALSE],
                 y = train_df$Disorder_Subclass,
                 method = "lda",
                 preProc = c("center", "scale"),
                 metric = "ROC",
                 trControl = fit_control)
lda_cm <- confusionMatrix(lda_fit, norm = "none")
lda_cm

# Check variable importance
lda_imp <- varImp(lda_fit, scale = FALSE)
lda_imp
```


###### Logistic Regression Model

```{r lr_model, eval = lr_eval}
# Train LR model
set.seed(476)
invisible(capture.output(
  lr_fit <- train(x = train_df[ , -target_col, drop = FALSE],
                  y = train_df$Disorder_Subclass,
                  method = "multinom",
                  metric = "ROC",
                  trControl = fit_control)
))
lr_cm <- confusionMatrix(lr_fit, norm = "none")
lr_cm

# Check variable importance
lr_imp <- varImp(lr_fit, scale = FALSE)
plot(lr_imp, top = 10, main = "Top 10 Variables")
```


###### Nearest shrunken Centroids Model

```{r nsc_model, eval = nsc_eval}
# Train NSC model
set.seed(476)
invisible(capture.output(
  nsc_fit <- train(x = train_df[ , -target_col, drop = FALSE],
                   y = train_df$Disorder_Subclass,
                   method = "pam",
                   preProc = c("center", "scale"),
                   tuneGrid = data.frame(threshold = seq(0, 25, length = 30)),
                   metric = "ROC",
                   trControl = fit_control)
))
nsc_cm <- confusionMatrix(nsc_fit, norm = "none")
nsc_cm

# Check variable importance
nsc_imp <- varImp(nsc_fit, scale = FALSE)
plot(nsc_imp, top = 10, main = "Top 10 Variables")
```


###### Random Forest Model

```{r rf_mmodel, eval = rf_eval}
# Create Random Forest weight vector based on class priors
priors <- as.list(prop.table(table(train_df$Disorder_Subclass)))
wts <- data.frame(Disorder_Subclass = train_df$Disorder_Subclass, w = 0.0)
for (n in 1:length(priors))
  wts[wts$Disorder_Subclass == names(priors[n]), ]$w <- priors[[n]]

# Train the model (using defaults)
rf_fit <- randomForest(x = train_df[ , -target_col, drop = FALSE],
                       y = train_df$Disorder_Subclass,
                       xtest = test_df[ , -target_col, drop = FALSE],
                       ytest = test_df$Disorder_Subclass,
                       weights = as.vector(wts$w),
                       importance = TRUE)

# Simplify class names for more coherent confusion matrix, and output
for (n in 1:length(rownames(rf_fit$confusion)))
  rownames(rf_fit$confusion)[n] <- paste(rownames(rf_fit$confusion)[n], " (", AscToChar(64 + n), ")", sep = "")
for (n in 1:length(rownames(rf_fit$confusion)))
  colnames(rf_fit$confusion)[n] <- paste("Class", AscToChar(64 + n))
for (n in 1:length(rownames(rf_fit$test$confusion)))
  rownames(rf_fit$test$confusion)[n] <- paste(rownames(rf_fit$test$confusion)[n], " (", AscToChar(64 + n), ")", sep = "")
for (n in 1:length(rownames(rf_fit$test$confusion)))
  colnames(rf_fit$test$confusion)[n] <- paste("Class", AscToChar(64 + n))

# Check variable importance
rf_imp <- varImp(rf_fit, scale = FALSE)
plot(rf_imp, top = 10, main = "Top 10 Variables")
```


###### CART Model

```{r cart_model, eval = cart_eval}
# Train CART model
set.seed(476)
cart_fit <- train(x = train_df[ , -target_col, drop = FALSE],
                  y = train_df$Disorder_Subclass,
                  method = "rpart",
                  tuneLength = 30,
                  metric = "ROC",
                  trControl = fit_control)
cart_cm <- confusionMatrix(cart_fit, norm = "none")
cart_cm

# Check variable importance
cart_imp <- varImp(cart_fit, scale = FALSE)
plot(cart_imp, top = 10, main = "Top 10 Variables")
```


###### Bagged Trees Model

```{r bt_model, eval = bt_eval}
# Train BT model
bt_fit <- train(x = train_df[ , -target_col, drop = FALSE],
                y = train_df$Disorder_Subclass,
                method = "treebag",
                metric = "ROC",
                importance = TRUE,
                trControl=fit_control)
bt_cm <- confusionMatrix(bt_fit, norm = "none")
bt_cm

# Check variable importance
bt_imp <- varImp(bt_fit, scale = FALSE)
plot(bt_imp, top = 10, main = "Top 10 Variables")
```


###### KNN Model

```{r knn_model, eval = knn_eval}
# Train KNN model
set.seed(476)
knn_fit <- train(x = train_df[ , -target_col, drop = FALSE],
                 y = train_df$Disorder_Subclass,
                 method = "knn",
                 metric = "ROC",
                 trControl = fit_control)
knn_cm <- confusionMatrix(knn_fit, norm = "none")
knn_cm

# Check variable importance
knn_imp <- varImp(knn_fit, scale = FALSE)
plot(knn_imp, top = 10, main = "Top 10 Variables")
```


###### Model Validation / Evaluation

```{r mmodel_validation}
# Validate models
if (lda_eval) {
  print("Linear Disrciminate Analysis")
  lda_pred <- predict(lda_fit, test_df[ , -target_col, drop = FALSE])
  lda_pred_cm = confusionMatrix(lda_pred, test_df$Disorder_Subclass)
  lda_pred_cm
}

if (lr_eval) {
  print("Logistic Regression")
  lr_pred <- predict(lr_fit, test_df[ , -target_col, drop = FALSE])
  lr_pred_cm = confusionMatrix(lr_pred, test_df$Disorder_Subclass)
  lr_pred_cm
}

if (nsc_eval) {
  print("Nearest Shrunken Centroids")
  nsc_pred <- predict(nsc_fit, test_df[ , -target_col, drop = FALSE])
  nsc_pred_cm = confusionMatrix(nsc_pred, test_df$Disorder_Subclass)
  nsc_pred_cm
}

if (rf_eval) {
  print("Random Forest")
  rf_fit
}

if (cart_eval) {
  print("CART")
  # Validate model vs. test data
  cart_pred <- predict(cart_fit, test_df[ , -target_col, drop = FALSE])
  cart_pred_cm = confusionMatrix(cart_pred, test_df$Disorder_Subclass)
  cart_pred_cm
}

if (bt_eval) {
  print("Bagged Trees")
  bt_pred <- predict(bt_fit, test_df[ , -target_col, drop = FALSE])
  bt_pred_cm = confusionMatrix(bt_pred, test_df$Disorder_Subclass)
  bt_pred_cm
}

if (knn_eval) {
  print("KNN")
  knn_pred <- predict(knn_fit, test_df[ , -target_col, drop = FALSE])
  knn_pred_cm = confusionMatrix(knn_pred, test_df$Disorder_Subclass)
  knn_pred_cm
}
```


```{r model_evaluation}
## Plot the ROC curve for the hold-out set
if (lda_eval) {
  lda_roc <- multiclass.roc(response = test_df$Disorder_Subclass,
                            predictor = order(lda_pred))
  plot.roc(lda_roc$rocs[[1]], type = "s", col = 'red', legacy.axes = TRUE,
           main = "Compare ROC Curves for All Models")
}

if (lr_eval) {
  lr_roc <- multiclass.roc(response = test_df$Disorder_Subclass,
                           predictor = order(lr_pred))
  plot.roc(lr_roc$rocs[[1]], type = "s", add = TRUE, col = 'green', legacy.axes = TRUE)
}

if (nsc_eval) {
  nsc_roc <- multiclass.roc(response = test_df$Disorder_Subclass,
                            predictor = order(nsc_pred))
  plot.roc(nsc_roc$rocs[[1]], type = "s", add = TRUE, col = 'blue', legacy.axes = TRUE)
}

if (rf_eval) {
  rf_roc <- multiclass.roc(response = test_df$Disorder_Subclass,
                           predictor = order(rf_fit$test$predicted))
  plot.roc(rf_roc$rocs[[1]], type = "s", col = 'orange', add = TRUE, legacy.axes = TRUE)
  #par(pty = "s")
  #lines <- sapply(2:length(rocs), function(x) lines.roc(rocs[[x]], col = x))
  #dev <- dev.off()
}

if (cart_eval) {
  cart_roc <- multiclass.roc(response = test_df$Disorder_Subclass,
                             predictor = order(cart_pred))
  plot.roc(cart_roc$rocs[[1]], type = "s",col = 'black', add = TRUE, legacy.axes = TRUE)
}

if (bt_eval) {
  bt_roc <- multiclass.roc(response = test_df$Disorder_Subclass,
                           predictor = order(bt_pred))
  plot.roc(bt_roc$rocs[[1]], type = "s", col = 'purple',add = TRUE, legacy.axes = TRUE)
}

if (knn_eval) {
  knn_roc <- multiclass.roc(response = test_df$Disorder_Subclass,
                            predictor = order(knn_pred))
  plot.roc(knn_roc$rocs[[1]], type = "s", col = 'yellow', add = TRUE, legacy.axes = TRUE)
}

if (lda_eval | lr_eval | nsc_eval | rf_eval | cart_eval | bt_eval | knn_eval)
  legend("bottomright", legend = c("LDA", "LR", "NSC", "RF", "CART", "BT", "Knn"),
         col = c("red", "green", "blue", "orange", "black", "purple", "yellow"), lwd = 2)
```
